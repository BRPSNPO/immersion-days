{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalando pacotes e libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyarrow) (1.18.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.0)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.7.1)\n",
      "Requirement already satisfied: boto3>=1.9.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.14.16)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.17.16)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.91->s3fs) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.91->s3fs) (0.3.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.25.8)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: simplejson in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (3.17.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install pyarrow\n",
    "!pip install s3fs\n",
    "!pip install simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando account name para criar o nome do bucket de forma Ãºnica todas as vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: accountid=877757629758\n",
      "env: bucket_name=lab-877757629758\n"
     ]
    }
   ],
   "source": [
    "import simplejson\n",
    "with open('/opt/ml/metadata/resource-metadata.json') as fh:\n",
    "    metadata = simplejson.loads(fh.read())\n",
    "accountid = metadata['ResourceArn'].split(':')[4]\n",
    "\n",
    "%set_env accountid={accountid}\n",
    "%set_env bucket_name=lab-{accountid}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando bucket para o Datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket exists. deleting bucket...\n",
      "make_bucket: lab-877757629758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (400) when calling the HeadBucket operation: Bad Request\n",
      "fatal error: An error occurred (NoSuchBucket) when calling the ListObjectsV2 operation: The specified bucket does not exist\n",
      "\n",
      "remove_bucket failed: Unable to delete all objects in the bucket, bucket will not be deleted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if [[ -z $(aws s3api head-bucket --bucket $bucket_name) ]]; then\n",
    "        echo \"bucket exists. deleting bucket...\"\n",
    "        aws s3 rb s3://$bucket_name --force \n",
    "else\n",
    "        echo \"bucket does not exist. creating bucket...\"\n",
    "fi\n",
    "\n",
    "aws s3 mb s3://$bucket_name --region us-east-2\n",
    "aws s3 ls s3://$bucket_name/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando pastas no bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\"\n",
      "}\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\"\n",
      "}\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3api put-object --bucket $bucket_name --key data/landing/\n",
    "aws s3api put-object --bucket $bucket_name --key data/staging/\n",
    "aws s3api put-object --bucket $bucket_name --key data/analytics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading file from movielens website...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/tmp/ml-1m.zip', <http.client.HTTPMessage at 0x7f892852c630>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"downloading file from movielens website...\")\n",
    "urllib.request.urlretrieve(\n",
    "        'http://files.grouplens.org/datasets/movielens/ml-1m.zip',\n",
    "        '/tmp/ml-1m.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting dataset into tmp folder...\n"
     ]
    }
   ],
   "source": [
    "print(\"extracting dataset into tmp folder...\")\n",
    "with ZipFile('/tmp/ml-1m.zip', 'r') as zipObj:\n",
    "   zipObj.extractall('/tmp/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realizando ETL e upload dos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200727_221815\n",
      "env: etl_date=20200727_221815\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "x = datetime.datetime.now()\n",
    "etl_date = x.strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(etl_date) \n",
    "%set_env etl_date={etl_date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../../../../tmp/ml-1m/movies.dat to s3://lab-877757629758/data/landing/movies/movies_20200727_221815.dat\n",
      "upload: ../../../../tmp/ml-1m/ratings.dat to s3://lab-877757629758/data/landing/ratings/ratings_20200727_221815.dat\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "aws s3 cp /tmp/ml-1m/movies.dat s3://$bucket_name/data/landing/movies/movies_$etl_date.dat\n",
    "aws s3 cp /tmp/ml-1m/ratings.dat s3://$bucket_name/data/landing/ratings/ratings_$etl_date.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv files...\n",
      "movies_df has 3883 lines\n"
     ]
    }
   ],
   "source": [
    "print(\"reading csv files...\")\n",
    "movies_df = pd.read_csv(\"/tmp/ml-1m/movies.dat\", \"::\", \n",
    "                        engine='python', \n",
    "                        header=None, \n",
    "                        names=['movieid', 'title', 'genres']) \n",
    "print(\"movies_df has %s lines\" % movies_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratings_df has 1000209 lines\n"
     ]
    }
   ],
   "source": [
    "ratings_df = pd.read_csv(\"/tmp/ml-1m/ratings.dat\", \"::\", \n",
    "                         engine='python', \n",
    "                         header=None, \n",
    "                         names=['userid', 'movieid', 'rating', 'timestamp']) \n",
    "print(\"ratings_df has %s lines\" % ratings_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging dataframes...\n"
     ]
    }
   ],
   "source": [
    "print(\"merging dataframes...\")\n",
    "merged_df = pd.merge(movies_df, ratings_df, on='movieid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating data...\n"
     ]
    }
   ],
   "source": [
    "print(\"aggregating data...\")\n",
    "aggregation_df = merged_df.groupby('title').agg({'rating': ['count', 'mean']})\n",
    "aggregation_df.columns = aggregation_df.columns.droplevel(level=0)\n",
    "aggregation_df = aggregation_df.rename(columns={\n",
    "    \"count\": \"rating_count\", \"mean\": \"rating_mean\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorting data...\n"
     ]
    }
   ],
   "source": [
    "print(\"sorting data...\")\n",
    "aggregation_df = aggregation_df.sort_values(\n",
    "        'rating_mean', \n",
    "        ascending=False).loc[aggregation_df['rating_count'] > 1000].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing files to s3...\n"
     ]
    }
   ],
   "source": [
    "print(\"writing files to s3...\")\n",
    "\n",
    "movies_df.to_parquet(\n",
    "        \"s3://\" + \n",
    "        os.getenv('bucket_name') + \n",
    "        \"/data/analytics/movies/movies_\" + \n",
    "        etl_date + \n",
    "        \".parquet.snappy\")\n",
    "\n",
    "ratings_df.to_parquet(\n",
    "        \"s3://\" + \n",
    "        os.getenv('bucket_name') + \n",
    "        \"/data/analytics/ratings/ratings_\" +\n",
    "        etl_date + \n",
    "        \".parquet.snappy\")\n",
    "\n",
    "aggregation_df.to_parquet(\n",
    "        \"s3://\" + \n",
    "        os.getenv('bucket_name') + \n",
    "        \"/data/analytics/best_movies/best_movies_\" +\n",
    "        etl_date + \n",
    "        \".parquet.snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fazendo leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file from s3 and printing result...\n",
      "result_df has 10 lines\n"
     ]
    }
   ],
   "source": [
    "print(\"reading file from s3 and printing result...\")\n",
    "result_df = pd.read_parquet(\n",
    "        \"s3://\" + \n",
    "        os.getenv('bucket_name') + \n",
    "        \"/data/analytics/best_movies/best_movies_\" + etl_date + \".parquet.snappy\")\n",
    "print(\"result_df has %s lines\" % result_df.size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
